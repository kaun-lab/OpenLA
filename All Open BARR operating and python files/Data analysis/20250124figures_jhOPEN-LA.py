# -*- coding: utf-8 -*-
"""Figures_JHedits.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QU75he5mS81aEl-9u8nr4skrthRgD8cP

# Set-up
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
import scipy as sp
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels
from scipy.stats import sem

import itertools
from statsmodels.sandbox.stats.multicomp import multipletests

import matplotlib
matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['ps.fonttype'] = 42

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

colors_1 = ['#D90429', '#B80422', '#9A031C']  #if experiemnts were run across multiple days, choose different HTML color codes for each day
colors_2 = ['#8D99AE', '#73829C', '#5E6C84'] 
colors_3 = ['#1D4298', '#183881', '#142E6B'] 


t, w, ang = 0.8, 0.25, 20

conditions = ['acv', 'air', 'benz'] #    
days = ['D1', 'D2', 'D3']

"""# Load Data (John)

## Tracking Data
"""

tracking = {}
for condition in conditions:
  temp_days = {}
  for day in days:
    temp = pd.read_csv(f'file path and naume.csv')
    temp_days[day] = temp
  tracking[condition] = temp_days

"""## Bouts Data"""

#05/06/2024 - version
bouts = {}
for condition in conditions:
  d1 = pd.read_csv(f'file path and name.csv')
  d2 = pd.read_csv(f'/file path and name.csv')
  d3 = pd.read_csv(f'file path and name.csv')

  d1['day'] = 'D1'
  d2['day'] = 'D2'
  d3['day'] = 'D3'

  combined = pd.concat([d1, d2, d3], ignore_index=True)
  combined['dur'] = combined['end'] - combined['start']
  bouts[condition] = combined

# D1R2_RNAi = bouts['Dop1R2_RNAi_MB027B'] #was acv
# D1R2_Gal4RNAi_control = bouts['Dop1R2_RNAi_Gal4_control']
# D1R2_UASRNAi_control = bouts['Dop1R2_RNAi_UAS_control']


# D1R2_RNAi['condition'] = 'Dop1R2_RNAi_MB027B' #was odor before condition
# D1R2_Gal4RNAi_control['condition'] = 'Dop1R2_RNAi_Gal4_control'
# D1R2_UASRNAi_control['condition'] = 'Dop1R2_RNAi_UAS_control'



# for i in range(1,4):
#     var_name='var_{}'.format(i)
#     list_of_names.append(var_name)
#     dict_final[list_of_names[i-1]]=dict_source[i]
# print(dict_source)
# print(dict_final)


list_of_names=[]
for i in range(1, len(conditions) +1):
 group_name = 'condition_{}'.format(i)
 list_of_names.append(group_name)
#  print(group_name)
 group_name = bouts[condition]
#  print(bouts[condition])



list_of_groups = []
for name, condition in zip(list_of_names, conditions):
  globals()[name] = bouts[condition]
  list_of_groups.append(globals()[name])
  for i, df in enumerate(list_of_groups):
   df['condition'] = conditions[i]
   #print(df['condition'].head())

combined = pd.concat(list_of_groups, ignore_index=True) #took out benz from []

samples_combined = combined[(combined['dur'] <= 2) & (combined['in'] == 1)]

"""## Behavioral Data"""

def combine(folder_path):
  df = pd.DataFrame([])
  for root, dirs, files in os.walk(folder_path):
    for file_name in files:
      df = pd.concat([df, pd.read_csv(os.path.join(root, file_name))], ignore_index=True)
  return df

samples_combined = combined[(combined['dur'] <= 2) & (combined['in'] == 1)]

pausing = combine('file path and name')
pausing['dur'] = pausing['end'] - pausing['start']

squiggles = combine('file path and name')
squiggles['dur'] = squiggles['end'] - squiggles['start']

"""### Depth Data"""

depth_pausing = pd.read_csv('file path and name.csv')
depth_pausing['dur'] = depth_pausing['end'] - depth_pausing['start']

depth_squiggles = pd.read_csv('file path and name.csv')
depth_squiggles['dur'] = depth_squiggles['end'] - depth_squiggles['start']

"""## Bout stuff"""

df = combined.copy()
from scipy.stats import sem
#just ROSA
df = df[df['in'] == 1]
path_to_saveplace = 'file path and name'
def analyze_and_plot_bout_durations_in_region_by_day_bar_graph(df):
    bins = [0, 2, 5, 10, 15, 20, 25, 30, float('inf')]
    labels = ['<2', '2-5', '5-10', '10-15', '15-20', '20-25', '25-30', '30+']
    df['duration_category'] = pd.cut(df['dur'], bins=bins, labels=labels, right=False) #Easier to define variable in one line like so
    grouped = df.groupby(['condition', 'day', 'id', 'duration_category']).size().reset_index(name='count') #conditions used to be odor here
    summary = grouped.groupby(['condition', 'day', 'duration_category']).agg(mean_count=('count', 'mean'), sem_count=('count', sem)).reset_index()
    plt.figure(figsize=(12, 6))
    palette = sns.color_palette("hsv", len(df['condition'].unique()))
    for day in df['day'].unique():
        plt.figure(figsize=(12, 8))
        day_data = summary[summary['day'] == day]
        categories = day_data['duration_category'].unique()
        x = np.arange(len(categories))
        bar_width = 0.15
        #colors = palette[:len(day_data['odor'].unique())]
        colors = ['#318787', '#819575'] #, '#E50715']
        for i, (condition, color) in enumerate(zip(day_data['condition'].unique(), colors)):
            condition_data = day_data[day_data['condition'] == condition]
            means = condition_data['mean_count']
            errors = condition_data['sem_count']
            plt.bar(x + i * bar_width, means, width=bar_width, label=condition, color=color, yerr=errors, capsize=3)
        plt.title(f'Day {day} - Average Number of Bouts per Duration Category by Experiment')
        plt.xticks(x + bar_width * (len(day_data['condition'].unique()) - 1) / 2, categories)
        plt.ylim(0, 6)
        plt.xlabel('Duration Category (seconds)')
        plt.ylabel('Average Count')
        plt.legend(title='Condition')
        #plt.grid(True)
        plt.savefig(os.path.join(path_to_saveplace, f'{day} Avg Count per Bout Duration.pdf'), transparent = True)
        plt.show()
        #return summary, grouped
analyze_and_plot_bout_durations_in_region_by_day_bar_graph(df) #summary, grouped =

#This code saves the dataframe
df = combined.copy()
df = df[df['in'] == 1]
day = 'D1'
df = df[df['day'] == day]
string_1 = 'odor_1' #
string_2 = 'odor_2'  #
path_to_saveplace = 'file path and name'


def analyze_bout_durations(df):
    bins = [0, 2, 5, 10, 15, 20, 25, 30, float('inf')]
    labels = ['<2', '2-5', '5-10', '10-15', '15-20', '20-25', '25-30', '30+']
    df['duration_category'] = pd.cut(df['dur'], bins=bins, labels=labels, right=False)
    #grouped = df.groupby(['condition', 'day', 'id', 'duration_category']).size().reset_index(name='count')
    grouped = df.groupby([ 'day', 'id', 'duration_category']).size().reset_index(name='count')
    # print("Grouped DataFrame:")
    # print(grouped.head())
    # summary = grouped.groupby(['condition', 'day', 'duration_category']).agg(mean_count=('count', 'mean'),
    #                                                                    sem_count=('count', sem),
    #                                                                    total_count=('count', 'sum')).reset_index()
    # print("Summary DataFrame:")
    # print(summary.head())
    return grouped
grouped = analyze_bout_durations(df)
grouped.to_csv(os.path.join(path_to_saveplace, 'Summary of Bout Types ' + day + '.csv'))


condition_1_bout_types = grouped[grouped['id'].str.contains(string_1)]
condition_2_bout_types = grouped[grouped['id'].str.contains(string_2)]

condition_1_bout_types.to_csv(os.path.join(path_to_saveplace, string_1 + ' Summary of Bout Types '+ f'{day}.csv'))
condition_2_bout_types.to_csv(os.path.join(path_to_saveplace, string_2 + ' Summary of Bout Types '+ f'{day}.csv'))

#summary.to_csv(os.path.join(path_to_saveplace, 'Summary of Bout Types.csv'))

"""# Figure 2"""

def plot0(condition_1, condition_2, condition_3, ylim_top=None, ylab='DEFAULT Y', plot_title='DEFAULT TITLE'): #if the conditions you are analyzing changes, alter the input and the np.arrange accordingly
  x = np.arange(3)*w #change the 3 to a 2 if you only want to do ACV and benz
  fig, ax = plt.subplots(figsize=(2, 8)) #was 2,8 with air but changed to 2,6 when trying to just plot ACV and BENZ
  day, day_i, = 'D1', 0
  #print(day)

  condition_1_means = condition_1.groupby(by='day').mean()
  condition_2_means = condition_2.groupby(by='day').mean()
  condition_3_means = condition_3.groupby(by='day').mean()

  bars = []
  #bars.append(ax.bar(0*w, means_acv[day], w, alpha=t, color=colors_acv[day_i]))
  #bars.append(ax.bar(1*w, means_air[day], w, alpha=t, color=colors_air[day_i]))
  #bars.append(ax.bar(2*w, means_benz[day], w, alpha=t, color=colors_benz[day_i]))

  ax.scatter(0*w+np.random.uniform(-0.05, 0.05, len(condition_1.loc[day])), condition_1.loc[day], alpha=t, color=colors_1[day_i])
  ax.scatter(1*w+np.random.uniform(-0.05, 0.05, len(condition_2.loc[day])), condition_2.loc[day], alpha=t, color=colors_2[day_i])
  ax.scatter(2*w+np.random.uniform(-0.05, 0.05, len(condition_3.loc[day])), condition_3.loc[day], alpha=t, color=colors_3[day_i])


  ax.set_xticks(x, conditions) #Add 'Air' in the middle if including air took out 'Benz'
  ax.set_ylim(0, ylim_top)
  ax.set_ylabel(ylab)
  ax.set_title(plot_title)
  ax.spines[['right', 'top']].set_visible(False)
  plt.xticks(rotation=90) #This is where you change the angle of the labels

  fig.tight_layout()
  fig.patch.set_alpha(0)
  ax.patch.set_alpha(0)

  save = pd.concat([condition_1[day].reset_index().assign(condition=f'{conditions[0]}'),
                    condition_2[day].reset_index().assign(condition=f'{conditions[1]}'),
                    condition_3[day].reset_index().assign(condition=f'{conditions[2]}')], ignore_index=True) 
  save.to_csv(f'file path and name.csv', index=False)
  fig.savefig(f'file path and name.pdf', transparent=True)
  #print(day)

  #print(sp.stats.kruskal(metric_acv[day], metric_air[day], metric_benz[day]))
  #one = sp.stats.mannwhitneyu(metric_acv[day], metric_air[day])
  #two = sp.stats.mannwhitneyu(metric_acv[day], metric_benz[day])
  #three = sp.stats.mannwhitneyu(metric_benz[day], metric_air[day])
  #print('ACV vs Air', one)
  #print('ACV vs Benz', two)
  #print('Benz vs Air', three)
  #print('Bonferroni', multipletests([one[1], two[1], three[1]], alpha=0.5, method='bonferroni'))

"""## Time in ROSA (s)"""

plot0(condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['dur'].sum(),
      condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['dur'].sum(),
      condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['dur'].sum(),
      ylim_top=920,
      ylab=f'Title',
      plot_title=f'Day 1')

# avg time in ROSA
plot0(condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['dur'].mean(),
      condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['dur'].mean(),
      condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['dur'].mean(),
       ylim_top=250, #was 175
       ylab=f'Title',
       plot_title=f'Day 1')

"""## Time in RONSA (s)"""

plot0(condition_1[condition_1['in']==0].groupby(by=['day', 'id'])['dur'].sum(),
      condition_2[condition_2['in']==0].groupby(by=['day', 'id'])['dur'].sum(),
      condition_3[condition_3['in']==0].groupby(by=['day', 'id'])['dur'].sum(),
      ylim_top=920,
      ylab='Title',
      plot_title='Day 1')

# avg time in RONSA
plot0(condition_1[condition_1['in']==0].groupby(by=['day', 'id'])['dur'].mean(),
      condition_2[condition_2['in']==0].groupby(by=['day', 'id'])['dur'].mean(),
       condition_3[condition_3['in']==0].groupby(by=['day', 'id'])['dur'].mean(),
      ylim_top=250,
      ylab='Title',
      plot_title='Day 1')

"""## Distance in ROSA (cm)"""

plot0(condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['distance'].sum()/50,
      condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['distance'].sum()/50,
      #condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['distance'].sum()/50,
      ylim_top=400,
      ylab='Title',
      plot_title='Day 1')

# avg distance in ROSA
plot0(condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['distance'].mean()/50,
      condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['distance'].mean()/50,
      #condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['distance'].mean()/50,
      ylim_top=100,
      ylab='Title',
      plot_title='Day 1')

"""## Distance in RONSA (cm)"""

plot0(condition_1[condition_1['in']==0].groupby(by=['day', 'id'])['distance'].sum()/50,
      condition_2[condition_2['in']==0].groupby(by=['day', 'id'])['distance'].sum()/50,
      condition_3[condition_3['in']==0].groupby(by=['day', 'id'])['distance'].sum()/50,
      ylim_top=600,
      ylab='Title',
      plot_title='Day 1')

# avg time in RONSA
plot0(condition_1[condition_1['in']==0].groupby(by=['day', 'id'])['distance'].mean()/50,
      condition_2[condition_2['in']==0].groupby(by=['day', 'id'])['distance'].mean()/50,
      condition_3[condition_3['in']==0].groupby(by=['day', 'id'])['distance'].mean()/50,
      ylim_top=170,
      ylab='Title',
      plot_title='Day 1')

"""## Velocity ROSA (cm/s)"""

plot0((condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['distance'].sum()/50)/(condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['dur'].sum()),
      (condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['distance'].sum()/50)/(condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['dur'].sum()),
      #(condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['distance'].sum()/50)/(condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['dur'].sum()),
      ylim_top=2,
      ylab='Title',
      plot_title='Day 1')

# avg time in ROSA
plot0((condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['distance'].mean()/50)/(condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['dur'].mean()),
      (condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['distance'].mean()/50)/(condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['dur'].mean()),
      #(condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['distance'].mean()/50)/(condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['dur'].mean()),
      ylim_top=1,
      ylab='Title',
      plot_title='Day 1')

"""## Velocity in RONSA (cm/s)"""

plot0((condition_1[condition_1['in']==0].groupby(by=['day', 'id'])['distance'].sum()/50)/(condition_1[condition_1['in']==0].groupby(by=['day', 'id'])['dur'].sum()),
      (condition_2[condition_2['in']==0].groupby(by=['day', 'id'])['distance'].sum()/50)/(condition_2[condition_2['in']==0].groupby(by=['day', 'id'])['dur'].sum()),
      #(condition_3[condition_3['in']==0].groupby(by=['day', 'id'])['distance'].sum()/50)/(condition_3[condition_3['in']==0].groupby(by=['day', 'id'])['dur'].sum()),
      ylim_top=2,
      ylab='Title',
      plot_title='Day 1')

# avg time in RONSA
plot0((condition_1[condition_1['in']==0].groupby(by=['day', 'id'])['distance'].mean()/50)/(condition_1[condition_1['in']==0].groupby(by=['day', 'id'])['dur'].mean()),
      (condition_2[condition_2['in']==0].groupby(by=['day', 'id'])['distance'].mean()/50)/(condition_2[condition_2['in']==0].groupby(by=['day', 'id'])['dur'].mean()),
      #(condition_3[condition_3['in']==0].groupby(by=['day', 'id'])['distance'].mean()/50)/(condition_3[condition_3['in']==0].groupby(by=['day', 'id'])['dur'].mean()),
      ylim_top=1,
      ylab='Title',
      plot_title='Day 1')

"""## Total Entries"""

plot0(condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['in'].sum(),
      condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['in'].sum(),
      #condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['in'].sum(),
       ylim_top = 75,
      ylab='Title',
      plot_title='Day 1')

"""## Pausing"""

def plot_ratio(metric_acv, metric_air, metric_benz, ylim_top=None, ylab='DEFAULT Y', plot_title='DEFAULT TITLE'):
  x = np.arange(3)*w
  fig, ax = plt.subplots(figsize=(3, 8))
  day, day_i = 'D1', 0

  # means_acv = metric_acv.groupby(by='day').mean()
  # means_air = metric_air.groupby(by='day').mean()
  # means_benz = metric_benz.groupby(by='day').mean()

  # bars = []
  #bars.append(ax.bar(0*w, means_acv[day], w, alpha=t, color=colors_acv[day_i]))
  #bars.append(ax.bar(1*w, means_air[day], w, alpha=t, color=colors_air[day_i]))
  #bars.append(ax.bar(2*w, means_benz[day], w, alpha=t, color=colors_benz[day_i]))

  ax.scatter(0*w+np.random.uniform(-0.05, 0.05, len(metric_acv)), metric_acv['ratio'], alpha=t, color=colors_acv[day_i])
  ax.scatter(1*w+np.random.uniform(-0.05, 0.05, len(metric_air)), metric_air['ratio'], alpha=t, color=colors_air[day_i])
  ax.scatter(2*w+np.random.uniform(-0.05, 0.05, len(metric_benz)), metric_benz['ratio'], alpha=t, color=colors_benz[day_i])

  ax.set_xticks(x, ['ACV', 'Air', 'BENZ'])
  ax.set_ylim(0, ylim_top)
  ax.set_ylabel(ylab)
  ax.set_title(plot_title)
  ax.spines[['right', 'top']].set_visible(False)

  fig.tight_layout()
  fig.patch.set_alpha(0)
  ax.patch.set_alpha(0)

  # save = pd.concat([metric_acv[day].reset_index().assign(odor='acv'),
  #                   metric_air[day].reset_index().assign(odor='air'),
  #                   metric_benz[day].reset_index().assign(odor='benz')], ignore_index=True)
  # save.to_csv(f'/content/drive/MyDrive/Kaun_Hernandez lab shared folder/Open BARR  odor learning and memory paper 2023/Code & Data/Data Files for John/Figure 2/{day}/{ylab}.csv', index=False)
  # fig.savefig(f'/content/drive/MyDrive/Kaun_Hernandez lab shared folder/Open BARR  odor learning and memory paper 2023/Code & Data/Figures/Figure 2/{ylab}.pdf', transparent=True)

  # print(sp.stats.kruskal(metric_acv[day], metric_air[day], metric_benz[day]))
  # one = sp.stats.mannwhitneyu(metric_acv[day], metric_air[day])
  # two = sp.stats.mannwhitneyu(metric_acv[day], metric_benz[day])
  # three = sp.stats.mannwhitneyu(metric_benz[day], metric_air[day])
  # print('ACV vs Air', one)
  # print('ACV vs Benz', two)
  # print('Benz vs Air', three)
  # print('Bonferroni', multipletests([one[1], two[1], three[1]], alpha=0.5, method='bonferroni'))

temp = depth_pausing.copy()
temp = temp[temp['day']=='D1']
temp['bins'], bins = pd.cut(temp['start depth'], 200*np.arange(start=-1, stop=2), labels=['RONSA', 'ROSA'], include_lowest=True, retbins=True)
temp = temp.groupby(by=['odor', 'day', 'id', 'bins'], observed=True).sum().reset_index()
#temp.to_csv(f'/content/drive/MyDrive/Nelson/Paper/Data Files for John/Figure 2/D3/Pausing.csv', index=False)

ids = temp['id'].unique()
ratio = pd.DataFrame([], columns=['odor', 'id', 'ratio'])
excluded = []
for id in ids:
  temp_fly = temp[temp['id'] == id]
  if len(temp_fly) == 2:
    # print(temp_fly)
    ratio.loc[len(ratio)] = [temp_fly.reset_index().at[0, 'odor'], id, temp_fly[temp_fly['bins']=='ROSA']['dur'].reset_index(drop=True)[0]/temp_fly[temp_fly['bins']=='RONSA']['dur'].reset_index(drop=True)[0]]
  else:
    excluded.append(id)
ratio

plot_ratio(ratio[ratio['odor']=='acv'],
      ratio[ratio['odor']=='air'],
      ratio[ratio['odor']=='benz'],
      ylab='Pausing Ratio',
      ylim_top = 40,
      plot_title='Day 1')

ratio.to_csv('file path and name.csv', index=False)

#g = sns.FacetGrid(data=ratio, col='odor', hue='odor', palette=[colors_acv[0], colors_air[0], colors_benz[0]], height=8, aspect=0.4)
#g.map_dataframe(sns.barplot, 'bins', 'dur', estimator='mean', errorbar=None, alpha=t*0.5)
#g.map_dataframe(sns.swarmplot, 'bins', 'dur', size=6)
#g.set_titles(col_template='{col_name}')
#g.set(xlabel=None)
#g.set_ylabels('Total Pausing Time (s)')
plt.savefig(f'file path and name.pdf')

"""## Squiggles"""

temp = depth_squiggles.copy()
temp = temp[temp['day']=='D1']
temp['bins'], bins = pd.cut(temp['start depth'], 200*np.arange(start=-1, stop=2), labels=['RONSA', 'ROSA'], include_lowest=True, retbins=True)
temp = temp.groupby(by=['odor', 'day', 'id', 'bins'], observed=True).sum().reset_index()
#temp.to_csv('/content/drive/MyDrive/Kaun_Hernandez lab shared folder/Open BARR  odor learning and memory paper 2023/Code & Data/Data Files for John/Figure 2/Squiggles.csv', index=False)

#g = sns.FacetGrid(data=temp, col='odor', hue='odor', palette=[colors_acv[0], colors_air[0], colors_benz[0]], height=8, aspect=0.6)
# # g.map_dataframe(sns.barplot, 'bins', 'dur', estimator='mean', errorbar=None, alpha=t*0.75)
#g.map_dataframe(sns.swarmplot, 'bins', 'dur', size=6)
#g.set_titles(col_template='{col_name}')
#g.set(xlabel=None)
#g.set_ylabels('Total Squiggle Time (s)')


ids = temp['id'].unique()
ratio = pd.DataFrame([], columns=['odor', 'id', 'ratio'])
excluded = []
for id in ids:
  temp_fly = temp[temp['id'] == id]
  if len(temp_fly) == 2:
    # print(temp_fly)
    ratio.loc[len(ratio)] = [temp_fly.reset_index().at[0, 'odor'], id, temp_fly[temp_fly['bins']=='ROSA']['dur'].reset_index(drop=True)[0]/temp_fly[temp_fly['bins']=='RONSA']['dur'].reset_index(drop=True)[0]]
  else:
    excluded.append(id)
ratio

plot_ratio(ratio[ratio['odor']=='acv'],
      ratio[ratio['odor']=='air'],
      ratio[ratio['odor']=='benz'],
      ylab='Squiggles Ratio',
      ylim_top = 15,
      plot_title='Day 1')

ratio.to_csv('file path and name', index=False)

plt.savefig(f'file path and name.pdf')

"""## Other behaviors of interest"""

def plot1(condition_1, condition_2, condition_3, ylim_top=None, ylab='DEFAULT Y', plot_title='DEFAULT TITLE'): #metric_benz removed removed condition_3
  x = np.arange(3)*w #change the 3 to a 2 if you only want to do ACV and benz
  fig, ax = plt.subplots(figsize=(2, 8)) #was 2,8 with air but changed to 2,6 when trying to just plot ACV and BENZ
  day, day_i, = 'D1', 0
  #print(day)

  condition_1_means = condition_1.groupby(by='day').mean()
  condition_2_means = condition_2.groupby(by='day').mean()
  condition_3_means = condition_3.groupby(by='day').mean()

  bars = []
  #bars.append(ax.bar(0*w, means_acv[day], w, alpha=t, color=colors_acv[day_i]))
  #bars.append(ax.bar(1*w, means_air[day], w, alpha=t, color=colors_air[day_i]))
  #bars.append(ax.bar(2*w, means_benz[day], w, alpha=t, color=colors_benz[day_i]))

  ax.scatter(0*w+np.random.uniform(-0.05, 0.05, len(condition_1.loc[day])), condition_1.loc[day], alpha=t, color=colors_1[day_i])
  ax.scatter(1*w+np.random.uniform(-0.05, 0.05, len(condition_2.loc[day])), condition_2.loc[day], alpha=t, color=colors_2[day_i])
  ax.scatter(2*w+np.random.uniform(-0.05, 0.05, len(condition_3.loc[day])), condition_3.loc[day], alpha=t, color=colors_3[day_i])


  ax.set_xticks(x, conditions) #Add 'Air' in the middle if including air took out 'Benz'
  ax.set_ylim(0, ylim_top)
  ax.set_ylabel(ylab)
  ax.set_title(plot_title)
  ax.spines[['right', 'top']].set_visible(False)
  plt.xticks(rotation=90) #This is where you change the angle of the labels

  fig.tight_layout()
  fig.patch.set_alpha(0)
  ax.patch.set_alpha(0)

  save = pd.concat([condition_1[day].reset_index().assign(condition=f'{conditions[0]}'),
                    condition_2[day].reset_index().assign(condition=f'{conditions[1]}'),
                    condition_3[day].reset_index().assign(condition=f'{conditions[2]}')], ignore_index=True)
                      #Removed metric_benz[day].reset_index().assign(odor='benz') removed condition_3[day].reset_index().assign(condition=f'{conditions[2]}')],
  save.to_csv(f'file path and name.csv', index=False)
  fig.savefig(f'file path and name.pdf', transparent=True)
  #two = sp.stats.mannwhitneyu(metric_acv[day], metric_benz[day])
  #print('ACV vs AIR vs Benz', three) #was two before

plot1(condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['dur'].mean(),
      condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['dur'].mean(),
      #condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['dur'].mean(),   # metric_air= was before air when excluding air
      ylim_top=60,
      ylab='Figure title',
      plot_title='Day 1')

# nelson's suggested correction, uncomment if you want to use
# plot1(#acv[acv['in']==0].groupby(by=['day', 'id']).mean()['dur'],
#       benz[benz['in']==0].groupby(by=['day', 'id']).mean()['dur'],
#       air[air['in']==0].groupby(by=['day', 'id']).mean()['dur'],   # metric_air= was before air when excluding air
#       ylim_top=60,
#       ylab='Latency to Re-enter ROSA D1',
#       plot_title='Day 1')

plot1(condition_1[condition_1['in']==1].groupby(by=['day', 'id'])['depth'].mean()/50,
      condition_2[condition_2['in']==1].groupby(by=['day', 'id'])['depth'].mean()/50,
      condition_3[condition_3['in']==1].groupby(by=['day', 'id'])['depth'].mean()/50,
      ylab='Figure title',
      ylim_top=4,
      plot_title='Day 1')

plot1(condition_1[condition_1['in']==0].groupby(by=['day', 'id'])['depth'].mean()/50,
      condition_2[condition_2['in']==0].groupby(by=['day', 'id'])['depth'].mean()/50,
      condition_3[condition_3['in']==0].groupby(by=['day', 'id'])['depth'].mean()/50,
      ylab='Figure title',
      ylim_top=4,
      plot_title='Day 1')

s_con1 = samples_combined.copy()
s_con1 = s_con1[s_con1['condition']==conditions[0]]

s_con2 = samples_combined.copy()
s_con2 = s_con2[s_con2['condition']==conditions[1]]

s_con3 = samples_combined.copy()
s_con3 = s_con3[s_con3['condition']==conditions[2]]

plot1(s_con1[s_con1['in']==1].groupby(by=['day', 'id'])['in'].sum(),
      s_con2[s_con2['in']==1].groupby(by=['day', 'id'])['in'].sum(),
      s_con3[s_con3['in']==1].groupby(by=['day', 'id'])['in'].sum(),
      ylab='Samples D1',
      ylim_top=35,
      plot_title='Day 1')

"""# Figure 3"""

def add_0s(df, bins, metric): #When working with conditions > 2 you need to add metric after bins like this bins, metric
  ids = df['id'].unique()

  new_df = df.copy()
  for id in ids:
    temp_fly = df[df['id'] == id].reset_index(drop=True)
    if len(temp_fly) != 15: #Was 15 before when doing just 1 min bins, 90 for 10s bins
      factor = 1
      for bin in bins:
        temp_bin = temp_fly[temp_fly['start bins'] == bin]
        if temp_bin.size == 0:
          concat_df = pd.DataFrame({'condition':temp_fly.at[0, 'condition'], 'id':id, 'start bins':bin, metric:0, 'factors':factor}, index = [0]) #code was index = [0, 1, 2, 3]
          new_df =pd.concat([new_df,concat_df], axis = 0, ignore_index=True)#,ignore_index = True) #new_df.append({'odor':temp_fly.at[0, 'odor'], 'id':id, 'start bins':bin, 'dur':0, 'factors':factor}, ignore_index=True) #Changed the 0 to 60 to fix issues with latency to re-enter or to 10 when doing 10 s bins
        factor += 1
  return new_df

# @title Quartile extractor
df = combined.copy()
save_path = 'file path'
#lok just at day 3
df_day1 = df[df['day'] == 'D1']
# find the sum of day 3 for each fly within each genotype/odor
df_day1_in = df_day1[df_day1['in'] == 1]
total_time_day1 = df_day1_in.groupby(['condition', 'id'])['dur'].sum().reset_index()
# define the quartiles
def assign_quartiles(group):
    if group.nunique() > 1:
        return pd.qcut(group, 4, labels=False, duplicates='drop') + 1
    else:
        return pd.Series([1] * len(group), index=group.index)
total_time_day1['quartile'] = total_time_day1.groupby('condition')['dur'].transform(assign_quartiles)
quartile_counts = total_time_day1.groupby(['condition', 'quartile'])['id'].nunique().reset_index()
quartile_counts.rename(columns={'id': 'unique_individuals'}, inplace=True)
# Save dataframes for each quartile
quartile_dataframes = {}
for odor in total_time_day1['condition'].unique():
    for quartile in total_time_day1['quartile'].unique():
        ids_in_quartile = total_time_day1[
            (total_time_day1['condition'] == condition) & (total_time_day1['quartile'] == quartile)
        ]['id']
        quartile_df = df[df['id'].isin(ids_in_quartile) & (df['condition'] == condition)]
        quartile_key = f'{condition}_Quartile_{quartile}'
        quartile_dataframes[f'{condition}_Quartile_{quartile}'] = quartile_df
        quartile_df.to_csv(os.path.join(save_path, f"{quartile_key}.csv"), index=False)
for key in quartile_dataframes:
    print(f"{key}:\n{quartile_dataframes[key]}\n")
    quartile_dataframes
print(quartile_counts)

# @title If you want to exclude non-engagers
df= combined.copy()
individuals_excluded = df.groupby(['id', 'day'])['in'].sum().reset_index()
individuals_excluded = individuals_excluded[individuals_excluded['in'] == 0]['id'].unique()
df_filtered = df[~df['id'].isin(individuals_excluded)]
df_day3 = df_filtered[df_filtered['day'] == 'D3']
df_day3_in = df_day3[df_day3['in'] == 1]
total_time_day3 = df_day3_in.groupby(['condition', 'id'])['dur'].sum().reset_index()
save_path = 'file path'
def assign_quartiles(group):
    if group.nunique() > 1:
        return pd.qcut(group, 4, labels=False, duplicates='drop') + 1
    else:
        return pd.Series([1] * len(group), index=group.index)
total_time_day3['quartile'] = total_time_day3.groupby('condition')['dur'].transform(assign_quartiles)
quartile_counts = total_time_day3.groupby(['condition', 'quartile'])['id'].nunique().reset_index()
quartile_counts.rename(columns={'id': 'unique_individuals'}, inplace=True)
quartile_dataframes = {}
for condition in total_time_day3['condition'].unique():
    for quartile in total_time_day3['quartile'].unique():
        ids_in_quartile = total_time_day3[
            (total_time_day3['condition'] == condition) & (total_time_day3['quartile'] == quartile)
        ]['id']
        quartile_df = df_filtered[df_filtered['id'].isin(ids_in_quartile) & (df_filtered['condition'] == condition)]
        quartile_key = f'{condition}_Quartile_{quartile}'
        quartile_dataframes[quartile_key] = quartile_df
        # Save each dataframe to CSV
        quartile_df.to_csv(os.path.join(save_path, f"Abstainers_Dropped_{quartile_key}.csv"), index=False)
for key in quartile_dataframes:
    print(f"{key}:\n{quartile_dataframes[key]}\n")
print(quartile_counts)

#@title Batch graph for quartiles
from scipy.stats import sem, kruskal, mannwhitneyu
from statsmodels.stats.multitest import multipletests
from numpy.lib.type_check import real_if_close
"""
quartile plots
"""
def plot0(condition_1, condition_2, condition_3, day='D1', ylim_top=None, ylab='DEFAULT Y', plot_title='DEFAULT TITLE'): #
    x = np.arange(3) * w #3 represents the number of conditions
    fig, ax = plt.subplots(figsize=(6, 8)). # 6 and 8 represent the figure size
    day_i = ['D1', 'D2', 'D3'].index(day)
    means_1 = condition_1.groupby(by='day').mean()
    means_2 = condition_2.groupby(by='day').mean()
    means_3 = condition_3.groupby(by='day').mean()
    bars = []
    bars.append(ax.bar(0 * w, means_1.loc[day], w, alpha=t, color=colors_1[day_i]))
    bars.append(ax.bar(1 * w, means_2.loc[day], w, alpha=t, color=colors_2[day_i]))
    bars.append(ax.bar(2 * w, means_3.loc[day], w, alpha=t, color=colors_3[day_i]))
    ax.scatter(0 * w + np.random.uniform(-0.05, 0.05, len(condition_1.loc[day])), condition_1.loc[day], alpha=t, color=colors_1[day_i])
    ax.scatter(1 * w + np.random.uniform(-0.05, 0.05, len(condition_2.loc[day])), condition_2.loc[day], alpha=t, color=colors_2[day_i])
    ax.scatter(2 * w + np.random.uniform(-0.05, 0.05, len(condition_3.loc[day])), condition_3.loc[day], alpha=t, color=colors_3[day_i])
    ax.set_xticks(x)
    ax.set_xticklabels(['odor_1, 'odor_2', 'odor_3'])
    ax.set_ylim(0, ylim_top)
    ax.set_ylabel(ylab)
    ax.set_title(plot_title)
    ax.spines[['right', 'top']].set_visible(False)
    fig.tight_layout()
    fig.patch.set_alpha(0)
    ax.patch.set_alpha(0)
    save = pd.concat([condition_1.loc[day].reset_index().assign(condition='MB027B_Activation'),
                      condition_2.loc[day].reset_index().assign(condition='MB027B_Control'),
                      condition_3.loc[day].reset_index().assign(condition='Dop2R-RI')], ignore_index=True)
    save.to_csv(f'{save_path}/{day}/{ylab}.csv', index=False)
    fig.savefig(f'{save_path}/{day}/{ylab}.pdf', transparent=True)
    kruskal_result = kruskal(condition_1.loc[day], condition_2.loc[day], condition_3.loc[day])
    one = mannwhitneyu(condition_1.loc[day], condition_2.loc[day])
    two = mannwhitneyu(condition_1.loc[day], condition_3.loc[day])
    three = mannwhitneyu(condition_3.loc[day], condition_2.loc[day])
    bonferroni_result = multipletests([one[1], two[1], three[1]], alpha=0.5, method='bonferroni')
    print(f'Kruskal-Wallis test for {ylab} on {day}:', kruskal_result)
    print(f'Mann-Whitney test (MB027B_Activation vs MB027B_Control) for {ylab} on {day}:', one)
    #print(f'Mann-Whitney test (Control vs RI) for {ylab} on {day}:', two)
    #print(f'Mann-Whitney test (RI vs RH) for {ylab} on {day}:', three)
    #print(f'Bonferroni correction for {ylab} on {day}:', bonferroni_result)
    stats_data = {
        'Test': ['Kruskal-Wallis', 'Mann-Whitney (odor_1 vs odor_2', 'Mann-Whitney (odor vs odor)', 'Mann-Whitney (odor vs odor', 'Bonferroni (odor vs odor)', 'Bonferroni (odor vs odor)', 'Bonferroni (odor vs odor)'],
        'Statistic': [kruskal_result.statistic, one.statistic, two.statistic, three.statistic, '', '', ''],
        'p-value': [kruskal_result.pvalue, one.pvalue, two.pvalue, three.pvalue, bonferroni_result[1][0], bonferroni_result[1][1], bonferroni_result[1][2]]
    }
    stats_df = pd.DataFrame(stats_data)
    stats_df.to_csv(os.path.join(save_path, f'{ylab}_stats.csv'), index=False)
"""
quartile plots
"""
def plot_for_each_quartile(quartile_dataframes, save_path):
    quartiles = ['Quartile_1', 'Quartile_2', 'Quartile_3', 'Quartile_4']
    days = ['D1', 'D2', 'D3']
    for quartile in quartiles:
        metric_1 = quartile_dataframes[f'{conditions[0]}_{quartile}']
        metric_2 = quartile_dataframes[f'{conditions[1]}_{quartile}']
        metric_3 = quartile_dataframes[f'{conditions[2]}_{quartile}']
        for day in days:
            plot0(metric_1[metric_1['in'] == 1].groupby(by=['day', 'id']).sum()['dur'],
                  metric_2[metric_2['in'] == 1].groupby(by=['day', 'id']).sum()['dur'],
                  metric_3[metric_3['in'] == 1].groupby(by=['day', 'id']).sum()['dur'],
                  day=day,
                  ylim_top=950,
                  ylab=f'Total Time in ROSA (s)',
                  plot_title=f'Total Time in ROSA (s) - {day} - {quartile}')
            plot0(metric_1[metric_1['in'] == 1].groupby(by=['day', 'id'])['dur'].mean(),
                  metric_2[metric_2['in'] == 1].groupby(by=['day', 'id'])['dur'].mean(),
                  metric_3[metric_3['in'] == 1].groupby(by=['day', 'id'])['dur'].mean(),
                  day=day,
                  ylim_top=250,
                  ylab=f'Avg. Time in ROSA (s)',
                  plot_title=f'Avg. Time in ROSA (s) - {day} - {quartile}')
            plot0(metric_1[metric_1['in']==0].groupby(by=['day', 'id']).sum()['dur'],
                  metric_2[metric_2['in']==0].groupby(by=['day', 'id']).sum()['dur'],
                  metric_3[metric_3['in']==0].groupby(by=['day', 'id']).sum()['dur'],
                  day = day,
                  ylim_top=950,
                  ylab='Total Time in RONSA (s)',
                  plot_title=f'Total Time in RONSA (s) - {day} - {quartile}')
            plot0(metric_1[metric_1['in']==0].groupby(by=['day', 'id'])['dur'].mean(),
                  metric_2[metric_2['in']==0].groupby(by=['day', 'id'])['dur'].mean(),
                  metric_3[metric_3['in']==0].groupby(by=['day', 'id'])['dur'].mean(),
                  day = day,
                  ylim_top=950,
                  ylab='Avg. Time in RONSA (s)',
                  plot_title=f'Avg. Time in RONSA (s) - {day} - {quartile}')
            plot0(metric_1[metric_1['in']==1].groupby(by=['day', 'id']).sum()['distance']/50,
                  metric_2[metric_2['in']==1].groupby(by=['day', 'id']).sum()['distance']/50,
                  metric_3[metric_3['in']==1].groupby(by=['day', 'id']).sum()['distance']/50,
                  day = day,
                  ylim_top=400,
                  ylab='Total Distance moved in ROSA (cm)',
                  plot_title=f'Total Distance Moved in ROSA (s) - {day} - {quartile}')
            plot0(metric_1[metric_1['in']==1].groupby(by=['day', 'id'])['distance'].mean()/50,
                  metric_2[metric_2['in']==1].groupby(by=['day', 'id'])['distance'].mean()/50,
                  metric_3[metric_3['in']==1].groupby(by=['day', 'id'])['distance'].mean()/50,
                  day = day,
                  ylim_top=220,
                  ylab='Avg. Distance moved in ROSA (cm)',
                  plot_title=f'Average Distance Moved in ROSA (s) - {day} - {quartile}')
            plot0(metric_1[metric_1['in']==0].groupby(by=['day', 'id']).sum()['distance']/50,
                  metric_2[metric_2['in']==0].groupby(by=['day', 'id']).sum()['distance']/50,
                  metric_3[metric_3['in']==0].groupby(by=['day', 'id']).sum()['distance']/50,
                  ylim_top = 700,
                  day = day,
                  ylab='Total Distance moved in RONSA (cm)',
                  plot_title=f'Total Distance Moved in RONSA (s) - {day} - {quartile}')
            plot0(metric_1[metric_1['in']==0].groupby(by=['day', 'id'])['distance'].mean()/50,
                  metric_2[metric_2['in']==0].groupby(by=['day', 'id'])['distance'].mean()/50,
                  metric_3[metric_3['in']==0].groupby(by=['day', 'id'])['distance'].mean()/50,
                  day = day,
                  ylim_top=220,
                  ylab='Avg. Distance moved in RONSA (cm)',
                  plot_title=f'Average Distance Moved in RONSA (s) - {day} - {quartile}')
            plot0(metric_1[metric_1['in']==1].groupby(by=['day', 'id']).sum()['in'],
                  metric_2[metric_2['in']==1].groupby(by=['day', 'id']).sum()['in'],
                  metric_3[metric_3['in']==1].groupby(by=['day', 'id']).sum()['in'],
                  day = day,
                  ylim_top = 200,
                  ylab='Total Entries to ROSA',
                  plot_title=f'Total Entries ROSA - {day} - {quartile}')
save_path = ''
plot_for_each_quartile(quartile_dataframes, save_path)

"""## Sampling"""

d = 'D1'

# samples = samples_combined.copy()
# samples = samples[(samples['day'] == d)] & (samples['odor']) #!= 'air')

samples = combined.copy()
samples = samples[(samples['dur'] >= 2) & (samples['dur'] < 10) & (samples['in'] == 1) & (samples['day'] == d)]

# samples = combined.copy()
# samples = samples[(samples['dur'] >= 10) & (samples['in'] == 1) & (samples['day'] == d)]

# samples.to_csv(f'/content/drive/MyDrive/Nelson/Paper/Data Files for John/Figure 3/{d}/Samples >=10s.csv', index=False)

g = sns.histplot(data=samples, x='start', hue='odor', palette=[colors_acv[0], colors_air[0]], bins=15, multiple="dodge")
g.set(xlabel='Start Time (s)', ylabel='Number of Occurrences')
g.figure.patch.set_alpha(0)
g.axes.patch.set_alpha(0)
g.axes.set_ylim(0, 300)
g.set(xlabel='Binned Start Time (s)', ylabel='Number of Occurrences', title = 'Samples Per Minute')
# g.legend(title='Odor')
sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 3/Sampling >10s.pdf', transparent=True)

"""## Time in ROSA and Latency to Re-Enter"""

bins = 60*np.arange(16) #Is 91 when doing the 10s bins 16 for 1 min bins, so change 60 to 10 nd 16 to 91 if changing
def splitter_prep1(df):
  bins = 60*np.arange(16)
  ret_df = df.copy()
  ret_df['start bins'] = pd.cut(ret_df['start'], bins, include_lowest=True)
  ret_df['end bins'] = pd.cut(ret_df['end'], bins, include_lowest=True)
  ret_df['split'] = np.not_equal(ret_df['start bins'], ret_df['end bins'])
  return ret_df

def splitter1(df, bins):
  to_split = df[df['split'] == True]

  splite = pd.DataFrame(columns=df.columns)
  for index, row in to_split.iterrows():
    if row['dur'] <= 60: #Was 60 before when doing min bins 10 when doing 10s bins
      splite.loc[index+0.1] = [row['bout'], row['start'], row['start bins'].right, row['id'], row['day'], row['start bins'].right-row['start'], row['condition'], row['start bins'], row['start bins'], False]
      splite.loc[index+0.2] = [row['bout'], row['start bins'].right, row['end'], row['id'], row['day'], row['end']-row['start bins'].right, row['condition'], row['end bins'], row['end bins'], False]
    else:
      first = True
      i = 0
      while bins[i] != row['start bins']:
        i += 1

      splite.loc[index+i*0.01] = [row['bout'], row['start'], row['start bins'].right, row['id'], row['day'], bins[i].right-row['start'], row['condition'], row['start bins'], row['start bins'], False]
      i += 1

      while bins[i] != row['end bins']:
        splite.loc[index+i*0.01] = [row['bout'], bins[i].left, bins[i].right, row['id'], row['day'], 60, row['condition'], bins[i], bins[i], False] #10 was 60 for 1 min bins
        i += 1

      splite.loc[index+i*0.01] = [row['bout'], bins[i].left, row['end'], row['id'], row['day'], row['end']-bins[i].left, row['condition'], row['end bins'], row['end bins'], False]

  return splite

d = 'D1'

temp = combined.copy()
temp = temp[temp['in'] == 0] #Switch 1 to 0 when you want to examine RONSA behavior
temp = temp.drop(columns=['in', 'depth', 'distance'])
temp = temp[(temp['day']==d)] #& (temp['odor'] != 'air')]


temp = splitter_prep1(temp)
bins = temp['start bins'].unique().sort_values()
print(bins)
splites = splitter1(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]
#temp['id'] = temp['id'].astype("string")
#temp['day'] = temp['day'].astype("string")
#temp['odor'] = temp['odor'].astype("string")

#numbers = temp.groupby(by=['odor', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers = temp.groupby(by=['condition', 'id', 'start bins'], observed=True)['dur'].sum().reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1
metric = 'dur'


# WHERE I CHANGED--------------
numbers = add_0s(numbers, bins, metric)
numbers = numbers.sort_values(['id', 'factors'])  # copy this line to reorganize the data such that the zeros are in their correct spot
# # -----------------------------

numbers.to_csv(f'file path and name.csv', index=False)
#time_1 = 0
#time_2 = 12
#numbers = numbers[time_1:time_2] #This is restricting the numbers dataset such that
#print(numbers)
#handles, labels = plt.gca() .get_legend_handles_labels()
# dagood = pd.DataFrame()
# for id in numbers['id'].unique():
#   idf = numbers[numbers['id'] == id]
#   idf = idf[0:12]
#   dagood = pd.concat([dagood, idf], axis = 0)
  #numbers[id] = numbers[id][numbers['factors'].isin(desired_factors)]
#plt.rcParams.update(plt.rcParamsDefault)
#plt.rcParams['figure.dpi'] = 600
#plt.rcParams['pdf.fonttype'] = 42
g = sns.lineplot(data=numbers, x='factors', y='dur', hue='condition', hue_order = ['Gr66a_ATR', 'Gr66a_noATR'], palette=[colors_1[1], colors_2[1]], errorbar='se') #took out colors_benz[0] removed colors_3[2]], data was numbers when doing 1 min bins
g.set(xlabel='Seconds', ylabel='Time in RONSA (s)')
g.set(xlabel='Binned Time by 1m', ylabel='Time in RONSA (s)', title = 'Time Spent In RONSA (s)')
g.figure.patch.set_alpha(0)
g.axes.patch.set_alpha(0)
#g.legend([handles[i] for i in order], [labels[i] for i in order])
g.legend(title='condition')
g.set_xticks(np.arange(1, 15)) #Make sure this is including the max x-axis datapoint. Change to 91 for 10s bins
g.set_ylim(15, 60)
g.set_xlim(1, 15) #can comment out for most graphing
sns.despine()

plt.savefig(f'file path and name}.pdf', transparent=True)

"""## Velocity in ROSA"""

d = 'D2'

temp = combined.copy()
temp = temp[temp['in'] == 1] #Switch 1 to 0 when you want to examine RONSA behavior

#temp = temp.drop(columns=['in', 'depth', 'distance'])
temp = temp.drop(columns=['in', 'depth'])
#dist = temp.groupby(by=['condition', 'id', 'start bins'], observed=True)['distance'].sum().reset_index()
#temp = temp.drop(columns=['distance'])
temp = temp[(temp['day']==d)] #& (temp['odor'] != 'air')]


temp = splitter_prep1(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter1(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]
#temp['id'] = temp['id'].astype("string")
#temp['day'] = temp['day'].astype("string")
#temp['odor'] = temp['odor'].astype("string")

#numbers = temp.groupby(by=['odor', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
#numbers = temp.groupby(by=['condition', 'id', 'start bins'], observed=True)(['dur'].sum())/((dist.sum()).reset_index())
#numbers = temp.groupby(by=['condition', 'id', 'start bins'], observed=True)['dur'].sum()/dist.reset_index()
numbers = temp.groupby(by=['condition', 'id', 'start bins'], observed=True)['dur'].sum()/temp.groupby(by=['condition', 'id', 'start bins'], observed=True)['distance'].sum().reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1


# WHERE I CHANGED--------------
numbers = add_0s(numbers, bins)
numbers = numbers.sort_values(['id', 'factors'])  # copy this line to reorganize the data such that the zeros are in their correct spot
# # -----------------------------

numbers.to_csv(f'file path and name.csv', index=False)

g = sns.lineplot(data=numbers, x='factors', y='dur', hue='condition', palette=[colors_1[0], colors_2[0]], errorbar='se') #took out colors_benz[0] and colors_3[0]
g.set(xlabel='Seconds', ylabel='Velocity in ROSA (cm_s)')
g.set(xlabel='Binned Time by Minute', ylabel='Velocity in RONSA (cm_s)', title = 'Velocity In RONSA (cm_s)')
g.figure.patch.set_alpha(0)
g.axes.patch.set_alpha(0)
g.legend(title='condition')
g.set_xticks(np.arange(1, 16)) #Make sure this is including the max x-axis datapoint. Change to 91 for 10s bins
g.set_ylim(0, 60)
sns.despine()
plt.savefig(f'file path and name.pdf', transparent=True)

d = 'D1'

temp = combined.copy()
temp = temp[temp['in'] == 0]
temp = temp.drop(columns=['in', 'depth', 'distance'])
temp = temp[(temp['day']==d)]# & (temp['odor'] != 'air')]
temp = splitter_prep1(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter1(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['condition', 'id', 'start bins'], observed=True)['dur'].mean().reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1


# numbers = temp.groupby(by=['odor', 'id', 'start bins'], observed=True).mean()['dur'].reset_index()
# numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
# numbers['factors'] = numbers['factors'] + 1

# WHERE I CHANGED--------------
numbers = add_0s(numbers, bins)
numbers = numbers.sort_values(['id', 'factors'])  # copy this line to reorganize the data such that the zeros are in their correct spot
# -----------------------------

numbers.to_csv(f'file path and name.csv', index=False)

g = sns.lineplot(data=numbers, x='factors', y='dur', hue='condition', palette=[colors_1[2], colors_2[2]], errorbar='se') #removed , colors_benz[0] colors_3[2]
g.set(xlabel='Binned Time by Minute', ylabel='Average Latency to Re-Enter (s)', title = 'Latency To Re-Enter ROSA')
g.legend(title='condition')
g.set_xticks(np.arange(1, 16))
g.set_ylim(6, 60)
sns.despine()
plt.savefig(f'file path and name.pdf', transparent=True)

"""## Squiggles and Pausing"""

def splitter_prep2(df):
  bins = 60*np.arange(16)
  ret_df = df.copy()
  ret_df['start bins'] = pd.cut(ret_df['start'], bins, include_lowest=True)
  ret_df['end bins'] = pd.cut(ret_df['end'], bins, include_lowest=True)
  ret_df['split'] = np.not_equal(ret_df['start bins'], ret_df['end bins'])
  return ret_df

def splitter2(df):
  to_split = df[df['split'] == True]

  splite = pd.DataFrame(columns=df.columns)
  for index, row in to_split.iterrows():
    if row['dur'] <= 60:
      splite.loc[index+0.1] = [row['id'], row['odor'], row['day'], row['start'], row['start bins'].right, row['start bins'].right-row['start'], row['start bins'], row['start bins'], False]
      splite.loc[index+0.2] = [row['id'], row['odor'], row['day'], row['start bins'].right, row['end'], row['end']-row['start bins'].right, row['end bins'], row['end bins'], False]
    else:
      first = True
      i = 0
      while bins[i] != row['start bins']:
        i += 1

      splite.loc[index+i*0.01] = [row['id'], row['odor'], row['day'], row['start'], row['start bins'].right, bins[i].right-row['start'], row['start bins'], row['start bins'], False]
      i += 1

      while bins[i] != row['end bins']:
        splite.loc[index+i*0.01] = [row['id'], row['odor'], row['day'], bins[i].left, bins[i].right, 60, bins[i], bins[i], False]
        i += 1

      splite.loc[index+i*0.01] = [row['id'], row['odor'], row['day'], bins[i].left, row['end'], row['end']-bins[i].left, row['end bins'], row['end bins'], False]

  return splite

d = 'D1'

temp = depth_pausing.copy()
# temp = temp[(temp['day']==d)].drop(columns=['start depth'])  # everywhere
temp = temp[(temp['day']==d) & (temp['start depth'] >= 0)].drop(columns=['start depth'])  # in ROSA
#temp = temp[(temp['day']==d) & (temp['start depth'] < 0)].drop(columns=['start depth'])  # in RONSA
temp = splitter_prep2(temp)
splites = splitter2(temp)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['odor', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

# WHERE I CHANGED--------------
numbers = add_0s(numbers, bins)
numbers = numbers.sort_values(['id', 'factor'])  # copy this line to reorganize the data such that the zeros are in their correct spot
# -----------------------------

numbers.to_csv(f'file path and name.csv', index=False)

g = sns.lineplot(data=numbers, x='factors', y='dur', hue='odor', palette=[colors_acv[0], colors_air[0], colors_benz[0]], errorbar='se')
g.set(xlabel='Binned Time by Minute', ylabel='Time Pausing (s)', title = 'Time Spent Pausing in ROSA')
g.legend(title='Odor')
g.set_xticks(np.arange(1, 16))
sns.despine()
plt.savefig(f'file path and name.pdf', transparent=True)

d = 'D1'

temp = depth_squiggles.copy()
# temp = temp[(temp['day']==d)].drop(columns=['start depth'])  # everywhere
temp = temp[(temp['day']==d) & (temp['start depth'] >= 0)].drop(columns=['start depth'])  # in ROSA
#temp = temp[(temp['day']==d) & (temp['start depth'] < 0)].drop(columns=['start depth'])  # in RONSA
temp = splitter_prep2(temp)
splites = splitter2(temp)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['odor', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

# WHERE I CHANGED--------------
numbers = add_0s(numbers, bins)
numbers = numbers.sort_values(['id', 'factor'])  # copy this line to reorganize the data such that the zeros are in their correct spot
# -----------------------------

numbers.to_csv(f'file path and name.csv', index=False)

g = sns.lineplot(data=numbers, x='factors', y='dur', hue='odor', palette=[colors_acv[0], colors_air[0], colors_benz[0]], errorbar='se')
g.set(xlabel='Time (Min)', ylabel='Time Odor-tracking (s)', title = 'Time Spent odor-tracking ROSA')
g.legend(title='Odor')
g.set_xticks(np.arange(1, 16))
sns.despine()
plt.savefig(f'file path and name.pdf', transparent=True)

"""## Distance in ROSA"""

def splitter_prep3(df):
  bins = bins = 60*np.arange(16) #60 goes to 10 and 16 goes to 91 for 10s bins
  ret_df = df.copy()
  ret_df['start bins'] = pd.cut(ret_df['start'], bins, include_lowest=True)
  ret_df['end bins'] = pd.cut(ret_df['end'], bins, include_lowest=True)
  ret_df['split'] = np.not_equal(ret_df['start bins'], ret_df['end bins'])
  return ret_df

def splitter3(df, bins):
  to_split = df[df['split'] == True]

  splite = pd.DataFrame(columns=df.columns)
  for index, row in to_split.iterrows():
    if row['end']-row['start'] <= 60: #Was 60 before when we were doing 1 min bins 10 for 10 s bins
      splite.loc[index+0.1] = [row['bout'], row['start'], row['start bins'].right, row['distance']*((row['start bins'].right-row['start'])/row['dur']), row['id'], row['day'], row['start bins'].right-row['start'], row['condition'], row['start bins'], row['start bins'], False]
      splite.loc[index+0.2] = [row['bout'], row['start bins'].right, row['end'], row['distance']*((row['end']-row['start bins'].right)/row['dur']), row['id'], row['day'], row['end']-row['start bins'].right, row['condition'], row['end bins'], row['end bins'], False]
    else:
      first = True
      i = 0
      while bins[i] != row['start bins']:
        i += 1

      splite.loc[index+i*0.01] = [row['bout'], row['start'], row['start bins'].right, row['distance']*((bins[i].right-row['start'])/row['dur']), row['id'], row['day'], bins[i].right-row['start'], row['condition'], row['start bins'], row['start bins'], False]
      i += 1

      while bins[i] != row['end bins']:
        splite.loc[index+i*0.01] = [row['bout'], bins[i].left, bins[i].right, row['distance']*(60/row['dur']), row['id'], row['day'], 60, row['condition'], bins[i], bins[i], False]
        i += 1

      splite.loc[index+i*0.01] = [row['bout'], bins[i].left, row['end'], row['distance']*((row['end']-bins[i].left)/row['dur']), row['id'], row['day'], row['end']-bins[i].left, row['condition'], row['end bins'], row['end bins'], False]

  return splite

d = 'D1'

temp = combined.copy()
temp = temp[temp['in'] == 1]
temp = temp.drop(columns=['in', 'depth'])
temp = temp[(temp['day']==d)] #& (temp['odor'] != 'air')]
temp = splitter_prep3(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter3(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['condition', 'id', 'start bins'], observed=True)['distance'].mean()/50
numbers = numbers.reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1
metric = 'distance'

# WHERE I CHANGED--------------
numbers = add_0s(numbers, bins, metric)
numbers = numbers.sort_values(['id', 'factors'])  # copy this line to reorganize the data such that the zeros are in their correct spot
# -----------------------------

numbers.to_csv(f'file path and name.csv', index=False)

# dagood = pd.DataFrame()
# for id in numbers['id'].unique():
#   idf = numbers[numbers['id'] == id]
#   idf = idf[0:12]
#   dagood = pd.concat([dagood, idf], axis = 0)

g = sns.lineplot(data=numbers, x='factors', y='distance', hue='condition', hue_order = ['Gr64f_ATR_ACV', 'Gr64f_noATR_ACV'], palette=[colors_1[1], colors_2[1]], errorbar='se') #removed , colors_benz[0], colors_3[2] changed data = numbers for the shorter timebins
g.figure.patch.set_alpha(0)
g.axes.patch.set_alpha(0)
g.set(xlabel='Binned Time by 1m', ylabel='Distance Moved in ROSA (cm)', title = 'Distance Moved In ROSA')
g.legend(title='condition')
g.set_xticks(np.arange(1, 15))
g.set_ylim(4, 13) #0,12 for ROSA for most exps
g.set_xlim(1, 15)
sns.despine()
plt.savefig(f'file path and name.pdf', transparent=True)

"""## Depth in ROSA"""

def splitter_prep4(df):
  bins = bins = 60*np.arange(16) #When doing 10s bins it's 10*np and np.arange(91)change 10 to 60 and 91 to 16 for mins
  ret_df = df.copy()
  ret_df['start bins'] = pd.cut(ret_df['start'], bins, include_lowest=True)
  ret_df['end bins'] = pd.cut(ret_df['end'], bins, include_lowest=True)
  ret_df['split'] = np.not_equal(ret_df['start bins'], ret_df['end bins'])
  return ret_df

def splitter4(df, bins):
  to_split = df[df['split'] == True]

  splite = pd.DataFrame(columns=df.columns)
  for index, row in to_split.iterrows():
    if row['end']-row['start'] <= 60:
      splite.loc[index+0.1] = [row['bout'], row['start'], row['start bins'].right, row['depth'], row['id'], row['day'], row['start bins'].right-row['start'], row['condition'], row['start bins'], row['start bins'], False]
      splite.loc[index+0.2] = [row['bout'], row['start bins'].right, row['end'], row['depth'], row['id'], row['day'], row['end']-row['start bins'].right, row['condition'], row['end bins'], row['end bins'], False]
    else:
      first = True
      i = 0
      while bins[i] != row['start bins']:
        i += 1

      splite.loc[index+i*0.01] = [row['bout'], row['start'], row['start bins'].right, row['depth'], row['id'], row['day'], bins[i].right-row['start'], row['condition'], row['start bins'], row['start bins'], False]
      i += 1

      while bins[i] != row['end bins']:
        splite.loc[index+i*0.01] = [row['bout'], bins[i].left, bins[i].right, row['depth'], row['id'], row['day'], 60, row['condition'], bins[i], bins[i], False]
        i += 1

      splite.loc[index+i*0.01] = [row['bout'], bins[i].left, row['end'], row['depth'], row['id'], row['day'], row['end']-bins[i].left, row['condition'], row['end bins'], row['end bins'], False]

  return splite

d = 'D1'

temp = combined.copy()
temp = temp[temp['in'] == 1]
temp = temp.drop(columns=['in', 'distance'])
temp = temp[(temp['day']==d)] #& (temp['odor'] != 'air')]
temp = splitter_prep4(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter4(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['condition', 'id', 'start bins'], observed=True)['depth'].mean()/50 #For the edit it will be ['depth'].mean instead of .mean
numbers = numbers.reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1
metric = 'depth'

# WHERE I CHANGED--------------
numbers = add_0s(numbers, bins, metric)
numbers = numbers.sort_values(['id', 'factors'])  # copy this line to reorganize the data such that the zeros are in their correct spot
# -----------------------------

numbers.to_csv(f'file path and name.csv', index=False)

# dagood = pd.DataFrame() #Command forward slash to comment out
# for id in numbers['id'].unique():
#   idf = numbers[numbers['id'] == id]
#   idf = idf[0:12]
#   dagood = pd.concat([dagood, idf], axis = 0)

g = sns.lineplot(data=numbers, x='factors', y='depth', hue='condition', hue_order = ['Gr64f_ATR_ACV', 'Gr64f_noATR_ACV'], palette=[colors_1[1], colors_2[1]], errorbar='se') #removed , colors_benz[0] removed colors_3[2]], , colors_3[2]
g.figure.patch.set_alpha(0)
g.axes.patch.set_alpha(0)
g.set(xlabel='Binned Time by 1m', ylabel='Avg. Depth in ROSA (cm)', title='Average Depth in ROSA')
g.legend(title='Condition')
g.set_ylim(1.5, 3.6)
g.set_xlim(1, 15)
g.set_xticks(np.arange(1, 15)) #Was 91 when we did 10s bins
# sns.despine()
plt.savefig(f'file path and name.pdf', transparent=True)

"""# Figure 4

## Time in ROSA
"""

temp = combined.copy()
temp = temp[(temp['in'] == 1) & (temp['odor'] == 'acv')]
temp = temp.drop(columns=['in', 'depth', 'distance'])
temp = splitter_prep1(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter1(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# g.set(xlabel='Time Binned by Minute', ylabel='Time in ROSA (s)', title='ACV - Time Spent in ROSA for Each Minute', ylim=(0, 45))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Time Spent in ROSA (s).pdf', transparent=True)

temp = combined.copy()
temp = temp[(temp['in'] == 1) & (temp['odor'] == 'air')]
temp = temp.drop(columns=['in', 'depth', 'distance'])
temp = splitter_prep1(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter1(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_air, errorbar='se')
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# g.set(xlabel='Time Binned by Minute', ylabel='Time in ROSA (s)', title='Air - Time Spent in ROSA for Each Minute', ylim=(0, 45))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/Air - Time Spent in ROSA (s).pdf', transparent=True)

temp = combined.copy()
temp = temp[(temp['in'] == 1) & (temp['odor'] == 'benz')]
temp = temp.drop(columns=['in', 'depth', 'distance'])
temp = splitter_prep1(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter1(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_benz, errorbar='se')
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# g.set(xlabel='Time Binned by Minute', ylabel='Time in ROSA (s)', title='BENZ - Time Spent in ROSA for Each Minute', ylim=(0, 45))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/BENZ - Time Spent in ROSA (s).pdf', transparent=True)

"""## Average Distance in ROSA"""

temp = combined.copy()
temp = temp[(temp['in'] == 1) & (temp['odor'] == 'acv')]
temp = temp.drop(columns=['in', 'depth'])
temp = splitter_prep3(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter3(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).mean()['distance'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='distance', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.set(xlabel='Time Binned by Minute', ylabel='Avg. Distance Moved in ROSA', title = 'ACV - Avg. Distance Moved In ROSA For Each Minute', ylim=(0, 600))
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Distance in ROSA.pdf', transparent=True)

temp = combined.copy()
temp = temp[(temp['in'] == 1) & (temp['odor'] == 'air')]
temp = temp.drop(columns=['in', 'depth'])
temp = splitter_prep3(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter3(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).mean()['distance'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='distance', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.set(xlabel='Time Binned by Minute', ylabel='Avg. Distance Moved in ROSA', title = 'ACV - Avg. Distance Moved In ROSA For Each Minute', ylim=(0, 600))
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Distance in ROSA.pdf', transparent=True)

temp = combined.copy()
temp = temp[(temp['in'] == 1) & (temp['odor'] == 'benz')]
temp = temp.drop(columns=['in', 'depth'])
temp = splitter_prep3(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter3(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).mean()['distance'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='distance', hue='day', style='day', palette=colors_benz, errorbar='se')
# g.set(xlabel='Time Binned by Minute', ylabel='Avg. Distance Moved in ROSA', title = 'BENZ - Avg. Distance Moved In ROSA For Each Minute', ylim=(0, 600))
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/Benz - Distance in ROSA.pdf', transparent=True)

"""## Latency To Re-Enter"""

temp = combined.copy()
temp = temp[(temp['in'] == 0) & (temp['odor'] == 'acv')]
temp = temp.drop(columns=['in', 'depth', 'distance'])
temp = splitter_prep1(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter1(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).mean()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.set(xlabel='Time Binned by Minute', ylabel='Average Latency to Re-Enter (s)', title='ACV - Latency To Re-Enter', ylim=(0, 20))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Latency to Re-Enter ROSA (s).pdf', transparent=True)

temp = combined.copy()
temp = temp[(temp['in'] == 0) & (temp['odor'] == 'air')]
temp = temp.drop(columns=['in', 'depth', 'distance'])
temp = splitter_prep1(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter1(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).mean()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.set(xlabel='Time Binned by Minute', ylabel='Average Latency to Re-Enter (s)', title='ACV - Latency To Re-Enter', ylim=(0, 20))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Latency to Re-Enter ROSA (s).pdf', transparent=True)

temp = combined.copy()
temp = temp[(temp['in'] == 0) & (temp['odor'] == 'benz')]
temp = temp.drop(columns=['in', 'depth', 'distance'])
temp = splitter_prep1(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter1(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).mean()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_benz, errorbar='se')
# g.set(xlabel='Time Binned by Minute', ylabel='Average Latency to Re-Enter (s)', title='BENZ - Latency To Re-Enter', ylim=(0, 20))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/BENZ - Latency to Re-Enter ROSA (s).pdf', transparent=True)

"""## Depth"""

temp = combined.copy()
temp = temp[(temp['in'] == 1) & (temp['odor'] == 'acv')]
temp = temp.drop(columns=['in', 'distance'])
temp = splitter_prep4(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter4(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).mean()['depth'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='depth', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.set(xlabel='Binned Time by Minute', ylabel='Max Depth in ROSA', title = 'Average Depth in ROSA For Each Minute', ylim=(0, 180))
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Depth in ROSA.pdf', transparent=True)

temp = combined.copy()
temp = temp[(temp['in'] == 1) & (temp['odor'] == 'air')]
temp = temp.drop(columns=['in', 'distance'])
temp = splitter_prep4(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter4(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).mean()['depth'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='depth', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.set(xlabel='Binned Time by Minute', ylabel='Max Depth in ROSA', title = 'Average Depth in ROSA For Each Minute', ylim=(0, 180))
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Depth in ROSA.pdf', transparent=True)

temp = combined.copy()
temp = temp[(temp['in'] == 1) & (temp['odor'] == 'benz')]
temp = temp.drop(columns=['in', 'distance'])
temp = splitter_prep4(temp)
bins = temp['start bins'].unique().sort_values()
splites = splitter4(temp, bins)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).mean()['depth'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='depth', hue='day', style='day', palette=colors_benz, errorbar='se', legend=False)
# g.set(xlabel='Binned Time by Minute', ylabel='Max Depth in ROSA', title = 'Average Depth in ROSA For Each Minute', ylim=(0, 180))
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/BENZ - Depth in ROSA.pdf', transparent=True)

"""## Sampling"""

samples = samples_combined.copy()
samples = samples[(samples['odor'] == 'acv')]

samples.to_csv(f'file path and name.csv', index=False)

# g = sns.histplot(data=samples, x='start', hue='day', palette=colors_acv, element='step', bins=15, legend=False)
# # g = sns.kdeplot(data=samples, x='start', hue='day')
# g.set(xlabel='Start Time (s)', ylabel='Number of Occurrences')
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Sampling.pdf', transparent=True)

samples = samples_combined.copy()
samples = samples[(samples['odor'] == 'air')]

samples.to_csv(f'file path and name.csv', index=False)

# g = sns.histplot(data=samples, x='start', hue='day', palette=colors_acv, element='step', bins=15, legend=False)
# # g = sns.kdeplot(data=samples, x='start', hue='day')
# g.set(xlabel='Start Time (s)', ylabel='Number of Occurrences')
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Sampling.pdf', transparent=True)

samples = samples_combined.copy()
samples = samples[(samples['odor'] == 'benz')]

samples.to_csv(f'file path and name.csv', index=False)

# g = sns.histplot(data=samples, x='start', hue='day', palette=colors_benz, element='step', bins=15, legend=False)
# # g = sns.kdeplot(data=samples, x='start', hue='day')
# g.set(xlabel='Start Time (s)', ylabel='Number of Occurrences')
# g.figure.patch.set_alpha(0)
# g.axes.patch.set_alpha(0)
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/BENZ - Sampling.pdf', transparent=True)

"""## Pausing"""

temp = depth_pausing.copy()
temp = temp[(temp['odor'] == 'acv')].drop(columns=['start depth'])  # everywhere
# temp = temp[(temp['odor'] == 'acv') & (temp['start depth'] >= 0)].drop(columns=['start depth'])  # in ROSA
# temp = temp[(temp['odor'] == 'acv') & (temp['start depth'] < 0)].drop(columns=['start depth'])  # in RONSA
temp = splitter_prep2(temp)
splites = splitter2(temp)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.set(xlabel='Binned Time by Minute', ylabel='Time Pausing (s)', title = 'Time Spent Pausing For Each Minute', ylim=(0, 35))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Pausing.pdf', transparent=True)

temp = depth_pausing.copy()
temp = temp[(temp['odor'] == 'air')].drop(columns=['start depth'])  # everywhere
# temp = temp[(temp['odor'] == 'air') & (temp['start depth'] >= 0)].drop(columns=['start depth'])  # in ROSA
# temp = temp[(temp['odor'] == 'air') & (temp['start depth'] < 0)].drop(columns=['start depth'])  # in RONSA
temp = splitter_prep2(temp)
splites = splitter2(temp)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.set(xlabel='Binned Time by Minute', ylabel='Time Pausing (s)', title = 'Time Spent Pausing For Each Minute', ylim=(0, 35))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Pausing.pdf', transparent=True)

temp = depth_pausing.copy()
temp = temp[(temp['odor'] == 'benz')].drop(columns=['start depth'])  # everywhere
# temp = temp[(temp['odor'] == 'benz') & (temp['start depth'] >= 0)].drop(columns=['start depth'])  # in ROSA
# temp = temp[(temp['odor'] == 'benz') & (temp['start depth'] < 0)].drop(columns=['start depth'])  # in RONSA
temp = splitter_prep2(temp)
splites = splitter2(temp)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_benz, errorbar='se')
# g.set(xlabel='Binned Time by Minute', ylabel='Time Pausing (s)', title = 'Time Spent Pausing For Each Minute', ylim=(0, 35))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/BENZ - Pausing.pdf', transparent=True)

"""## Squiggles"""

temp = depth_squiggles.copy()
temp = temp[(temp['odor'] == 'acv')].drop(columns=['start depth'])  # everywhere
# temp = temp[(temp['odor'] == 'acv') & (temp['start depth'] >= 0)].drop(columns=['start depth'])  # in ROSA
# temp = temp[(temp['odor'] == 'acv') & (temp['start depth'] < 0)].drop(columns=['start depth'])  # in RONSA
temp = splitter_prep2(temp)
splites = splitter2(temp)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.set(xlabel='Binned Time by Minute', ylabel='Time Squiggling (s)', title = 'Time Spent Squiggling For Each Minute', ylim=(0, 20))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Squiggling.pdf', transparent=True)

temp = depth_squiggles.copy()
temp = temp[(temp['odor'] == 'air')].drop(columns=['start depth'])  # everywhere
# temp = temp[(temp['odor'] == 'air') & (temp['start depth'] >= 0)].drop(columns=['start depth'])  # in ROSA
# temp = temp[(temp['odor'] == 'air') & (temp['start depth'] < 0)].drop(columns=['start depth'])  # in RONSA
temp = splitter_prep2(temp)
splites = splitter2(temp)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_acv, errorbar='se')
# g.set(xlabel='Binned Time by Minute', ylabel='Time Squiggling (s)', title = 'Time Spent Squiggling For Each Minute', ylim=(0, 20))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/ACV - Squiggling.pdf', transparent=True)

temp = depth_squiggles.copy()
temp = temp[(temp['odor'] == 'benz')].drop(columns=['start depth'])  # everywhere
# temp = temp[(temp['odor'] == 'benz') & (temp['start depth'] >= 0)].drop(columns=['start depth'])  # in ROSA
# temp = temp[(temp['odor'] == 'benz') & (temp['start depth'] < 0)].drop(columns=['start depth'])  # in RONSA
temp = splitter_prep2(temp)
splites = splitter2(temp)
temp = pd.concat([temp, splites], ignore_index=True)
temp = temp[temp['split'] == False]

numbers = temp.groupby(by=['day', 'id', 'start bins'], observed=True).sum()['dur'].reset_index()
numbers['factors'], bins = pd.factorize(numbers['start bins'], sort=True)
numbers['factors'] = numbers['factors'] + 1

numbers.to_csv(f'file path and name.csv', index=False)

# g = sns.lineplot(data=numbers, x='factors', y='dur', hue='day', style='day', palette=colors_benz, errorbar='se')
# g.set(xlabel='Binned Time by Minute', ylabel='Time Squiggling (s)', title = 'Time Spent Squiggling For Each Minute', ylim=(0, 20))
# g.legend(title='Day')
# g.set_xticks(np.arange(1, 16))
# sns.despine()
# plt.savefig(f'/content/drive/MyDrive/Nelson/Paper/Figures/Figure 4/BENZ - Squiggling.pdf', transparent=True)

"""# Preference Index"""

test = bouts['acv']
test = test[test['in'] == 0]
test.max()